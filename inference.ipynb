{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f383af7da603de",
   "metadata": {},
   "source": [
    "#!pip install geojson shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b967f2b-007d-4e37-9734-d2f1b3114c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version from inf_draft10_4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13cbf267-9bb3-4590-8d27-a0ffde30adc2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import openslide\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import geojson\n",
    "from geojson import Feature, FeatureCollection, Polygon\n",
    "import json\n",
    "#from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43025259-2496-409a-9e85-d6942256c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "YOLO_MODEL_PATH = 'yolo11_eosinophil_seg5/weights/best.pt'    # Path to YOLO11 pretrained model\n",
    "IMAGE_DIR = 'svs_inf'          # Input directory with SVS files\n",
    "OUTPUT_DIR = 'inference_output10'  # Output directory\n",
    "\n",
    "# Patch Extraction Parameters\n",
    "PATCH_SIZE = 448              # Patch dimensions\n",
    "PATCH_STRIDE = 424            # Stride between patches\n",
    "HPF_SIZE = 2144               # High-power field size\n",
    "HPF_STRIDE = 500              # Stride between HPFs\n",
    "\n",
    "# Prediction Output Mode (choose 'bbox', 'polygon', or 'both')\n",
    "PREDICTION_MODE = 'both'      # Options: 'bbox', 'polygon', 'both'\n",
    "\n",
    "CLASSES = ['eos', 'eosg', 'Tissue']  # Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d008403-612f-44ee-872b-423c65744623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_patches(slide, base_filename):\n",
    "    \"\"\"Extract patches from the slide according to the specified dimensions\"\"\"\n",
    "    width, height = slide.dimensions\n",
    "    \n",
    "    # Calculate how many high power fields (HPFs) we need\n",
    "    hpf_stride = HPF_STRIDE  # From the problem description 500 pixels\n",
    "    hpfs_x = max(1, (width - HPF_SIZE) // hpf_stride + 1)\n",
    "    hpfs_y = max(1, (height - HPF_SIZE) // hpf_stride + 1)\n",
    "    \n",
    "    patches_info = []\n",
    "    \n",
    "    for hpf_x in range(hpfs_x):\n",
    "        for hpf_y in range(hpfs_y):\n",
    "            # HPF coordinates\n",
    "            hpf_left = min(hpf_x * hpf_stride, width - HPF_SIZE)\n",
    "            hpf_top = min(hpf_y * hpf_stride, height - HPF_SIZE)\n",
    "            \n",
    "            # Now extract patches from this HPF\n",
    "            for patch_x in range(0, HPF_SIZE - PATCH_SIZE + 1, PATCH_STRIDE):\n",
    "                for patch_y in range(0, HPF_SIZE - PATCH_SIZE + 1, PATCH_STRIDE):\n",
    "                    patch_left = hpf_left + patch_x\n",
    "                    patch_top = hpf_top + patch_y\n",
    "                    \n",
    "                    # Create patch info\n",
    "                    patch_info = {\n",
    "                        'patch_left': patch_left,\n",
    "                        'patch_top': patch_top,\n",
    "                        'patch_name': f\"{base_filename}_hpf_{hpf_x}_{hpf_y}_patch_{patch_x}_{patch_y}\"\n",
    "                    }\n",
    "                    patches_info.append(patch_info)\n",
    "    \n",
    "    return patches_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4d6aa9-173e-4fcb-866f-39345879e87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patch(slide, patch_info):\n",
    "    \"\"\"Extract a single patch from the slide\"\"\"\n",
    "    patch_left = patch_info['patch_left']\n",
    "    patch_top = patch_info['patch_top']\n",
    "    \n",
    "    # Extract the patch as an image\n",
    "    patch_img = slide.read_region((patch_left, patch_top), 0, (PATCH_SIZE, PATCH_SIZE)) # method OpenSlide to extract a 448Ã—448 pixel region at level 0 (highest resolution)\n",
    "    patch_img = np.array(patch_img)  # Convert from PIL image to numpy array for further processing, feeding into YOLO\n",
    "    \n",
    "    # Convert the patch image to RGB (if necessary)\n",
    "    if patch_img.shape[2] == 4:  # If there's an alpha channel, remove it\n",
    "        patch_img = patch_img[:, :, :3]\n",
    "\n",
    "    # Outputs a clean RGB NumPy image array, ready for inference or visualization\n",
    "    return patch_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e81efa9-f751-4b16-80d3-58d88b0f1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VERSION FOR GEOJSON\n",
    "# we are using YOLO11 with task='segment', which means polygon masks are available via .masks.xy\n",
    "#from geojson import Feature, FeatureCollection, Polygon\n",
    "# Updated save_patch_results() function to import Polygon from the geojson library â€” not Shapely. \n",
    "# That fixes the shape compatibility with GeoJSON export.\n",
    "# Correct Import:\n",
    "# from geojson import Feature, FeatureCollection, Polygon\n",
    "# Ensure Shapely's Polygon is not being imported. Possible source confusion, because both packages have a Polygon class but with incompatible expectations.\n",
    "\n",
    "def save_patch_results(patch_img, results, patch_info, slide_output_dir, model):\n",
    "    result = results[0]\n",
    "\n",
    "    if result.boxes is None or len(result.boxes) == 0:\n",
    "        return\n",
    "\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()\n",
    "    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "    confidences = result.boxes.conf.cpu().numpy()\n",
    "    masks = result.masks.xy if hasattr(result, \"masks\") and result.masks is not None else None\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i, (box, class_id, score) in enumerate(zip(boxes, class_ids, confidences)):\n",
    "        class_name = model.names[class_id].lower()\n",
    "        if class_name not in [\"eos\", \"eosg\"]:\n",
    "            continue\n",
    "\n",
    "        # Convert box coords to float explicitly\n",
    "        x1, y1, x2, y2 = map(float, box)\n",
    "        x1 += patch_info['patch_left']\n",
    "        x2 += patch_info['patch_left']\n",
    "        y1 += patch_info['patch_top']\n",
    "        y2 += patch_info['patch_top']\n",
    "\n",
    "        bbox_coords = [\n",
    "            [float(round(x1, 2)), float(round(y1, 2))],\n",
    "            [float(round(x2, 2)), float(round(y1, 2))],\n",
    "            [float(round(x2, 2)), float(round(y2, 2))],\n",
    "            [float(round(x1, 2)), float(round(y2, 2))],\n",
    "            [float(round(x1, 2)), float(round(y1, 2))]\n",
    "        ]\n",
    "\n",
    "        if PREDICTION_MODE in ['bbox', 'both']:\n",
    "            features.append(Feature(\n",
    "                geometry=Polygon([bbox_coords]),  # For geojson.Polygon\n",
    "                properties={\n",
    "                    \"classification\": {\"name\": class_name},\n",
    "                    \"measurements\": [{\"name\": \"confidence\", \"value\": float(round(score, 4))}],\n",
    "                    \"source\": \"bbox\"\n",
    "                }\n",
    "            ))\n",
    "\n",
    "        if PREDICTION_MODE in ['polygon', 'both'] and masks is not None and i < len(masks):\n",
    "            mask_polygon = np.array(masks[i])\n",
    "            if mask_polygon.shape[0] >= 3:\n",
    "                mask_coords = [\n",
    "                    [float(round(x + patch_info['patch_left'], 2)), float(round(y + patch_info['patch_top'], 2))] \n",
    "                    for x, y in mask_polygon\n",
    "                ]\n",
    "                mask_coords.append(mask_coords[0])  # Ensure polygon is closed\n",
    "                features.append(Feature(\n",
    "                    geometry=Polygon([mask_coords]),  # For geojson.Polygon\n",
    "                    properties={\n",
    "                        \"classification\": {\"name\": class_name},\n",
    "                        \"measurements\": [{\"name\": \"confidence\", \"value\": float(round(score, 4))}],\n",
    "                        \"source\": \"polygon\"\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "    if not features:\n",
    "        return\n",
    "\n",
    "    feature_collection = FeatureCollection(features)\n",
    "    geojson_path = slide_output_dir / f\"{patch_info['patch_id']}_detections.geojson\"\n",
    "    with open(geojson_path, \"w\") as f:\n",
    "        geojson.dump(feature_collection, f, indent=2)\n",
    "\n",
    "    print(f\"Saved QuPath-compatible GeoJSON: {geojson_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db94673b-3b46-40a7-8a79-d1ad7f7ae1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new debug visualization\n",
    "\n",
    "def create_debug_visualization(patch_img, patch_info, results, slide_output_dir, model):\n",
    "    yolo_debug_img = patch_img.copy()\n",
    "    result = results[0]\n",
    "\n",
    "    if result.boxes is not None and len(result.boxes) > 0:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "        class_ids = result.boxes.cls.cpu().numpy().astype(int)\n",
    "        masks = result.masks.xy if hasattr(result, \"masks\") and result.masks is not None else None\n",
    "\n",
    "        for i, (box, score, class_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
    "            x1, y1, x2, y2 = box\n",
    "            class_name = model.names[class_id].lower()\n",
    "\n",
    "            if class_name not in ['eos', 'eosg']:\n",
    "                continue\n",
    "\n",
    "            if PREDICTION_MODE in ['bbox', 'both']:\n",
    "                cv2.rectangle(yolo_debug_img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                cv2.putText(yolo_debug_img, f\"{class_name} {score:.2f}\", \n",
    "                            (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            if PREDICTION_MODE in ['polygon', 'both'] and masks is not None and i < len(masks):\n",
    "                polygon = np.array(masks[i], dtype=np.int32)\n",
    "                cv2.polylines(yolo_debug_img, [polygon], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "\n",
    "        debug_id = patch_info.get('patch_id', f\"{patch_info.get('x', 0)}_{patch_info.get('y', 0)}\")\n",
    "        yolo_debug_path = slide_output_dir / f\"{debug_id}_yolo_debug.jpg\"\n",
    "        cv2.imwrite(str(yolo_debug_path), cv2.cvtColor(yolo_debug_img, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved debug image for patch {debug_id} to {yolo_debug_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e279b7-f011-4455-84a1-92a51955c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_yolo_inference_on_patches(model, batch_images, batch_ids, device='cuda', imgsz=448):\n",
    "    temp_dir = Path(\"temp_inference_patches\")\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    temp_image_paths = []\n",
    "    for i, img in enumerate(batch_images):\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        temp_path = temp_dir / f\"{batch_ids[i]}.jpg\"\n",
    "        cv2.imwrite(str(temp_path), img_bgr)\n",
    "        temp_image_paths.append(str(temp_path))\n",
    "\n",
    "    results = list(model.predict(\n",
    "        source=temp_image_paths,\n",
    "        imgsz=imgsz,\n",
    "        show=False,\n",
    "        stream=True,\n",
    "        device=device,\n",
    "        verbose=False\n",
    "    ))\n",
    "\n",
    "    # Clean up temporary images\n",
    "    # for f in temp_dir.glob(\"*.jpg\"):\n",
    "    #     f.unlink()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Updated inference pipeline\n",
    "\n",
    "model = YOLO(YOLO_MODEL_PATH, task='segment').to('cuda')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Process all SVS files\n",
    "svs_files = list(Path(IMAGE_DIR).glob(\"*.svs\"))\n",
    "for svs_path in tqdm(svs_files, desc=\"Processing slides\"):\n",
    "    slide_name = svs_path.stem\n",
    "    slide = openslide.OpenSlide(str(svs_path))\n",
    "    print(f\"Processing slide: {slide_name}\")\n",
    "\n",
    "    slide_output_dir = Path(OUTPUT_DIR) / slide_name\n",
    "    slide_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    patches_info = extract_patches(slide, slide_name)\n",
    "\n",
    "    batch_size = 16\n",
    "    # the number of patches, not batches in the print output\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(patches_info), batch_size), desc=f\"Processing batches for {slide_name}\"):\n",
    "        batch_end = min(batch_start + batch_size, len(patches_info))\n",
    "        current_batch_info = patches_info[batch_start:batch_end]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_ids = []\n",
    "        batch_info = []\n",
    "\n",
    "        for idx, patch_info in enumerate(current_batch_info):\n",
    "            patch_idx = batch_start + idx\n",
    "            patch_info['patch_id'] = f\"{slide_name}_patch_{patch_idx}\"\n",
    "            patch_img = get_patch(slide, patch_info)\n",
    "\n",
    "            if patch_img.mean() > 240: # If the patch_img.mean() is greater than 240 (on a scale of 0â€“255) -> the patch is almost entirely white, thus skip it\n",
    "                continue\n",
    "\n",
    "            batch_images.append(patch_img)\n",
    "            batch_ids.append(patch_info['patch_id'])\n",
    "            batch_info.append(patch_info)\n",
    "\n",
    "        if len(batch_images) == 0:\n",
    "            continue\n",
    "\n",
    "        # Run inference using safe wrapper with proper resizing\n",
    "        results = run_yolo_inference_on_patches(model, batch_images, batch_ids, device='cuda', imgsz=448)\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            patch_info = batch_info[i]\n",
    "            patch_img = batch_images[i]\n",
    "\n",
    "            if result.boxes is not None and len(result.boxes) > 0:\n",
    "                print(f\"Detection(s) in patch {patch_info['patch_id']}\")\n",
    "                # export to GeoJSON\n",
    "                save_patch_results(patch_img, [result], patch_info, slide_output_dir, model)\n",
    "                create_debug_visualization(patch_img, patch_info, [result], slide_output_dir, model)\n",
    "\n",
    "    slide.close()\n",
    "    print(f\"Finished processing slide: {slide_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4a360-ce47-406d-93a7-c8b147e02150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell below if for merging individual GeoJSON files for each detection to create merged GeoJSON fo rthe entire svs file (WSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aee547ed-7b03-4f48-b12b-306e58c15f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape, mapping\n",
    "from geojson import FeatureCollection, Feature\n",
    "import geojson\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Utility function that ensures objects (especially NumPy types) are compatible with JSON serialization \n",
    "# It is used for preparing complex Python data structures for output to files like .geojson\n",
    "def to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an object to a JSON-serializable format.\n",
    "    Primarily used to convert NumPy types and arrays to standard Python types\n",
    "    so they can be saved using `json.dump()` or `geojson.dump()`.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to be converted (can be a NumPy type, list, dict, etc.)\n",
    "\n",
    "    Returns:\n",
    "        A version of `obj` that is safe for JSON serialization.\n",
    "    \"\"\"\n",
    "    # Convert NumPy integer types to native Python int\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    # Convert NumPy float types to native Python float\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    # Convert NumPy arrays to native Python lists\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    # Recursively convert each item in a list\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_serializable(i) for i in obj]\n",
    "    # Recursively convert each value in a dictionary\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    # If already a basic type (e.g., str, int, float), return as-is\n",
    "    return obj\n",
    "\n",
    "def merge_geojsons(slide_output_dir, slide_name):\n",
    "    merged_features = []\n",
    "    geojson_files = sorted(slide_output_dir.glob(\"*_detections.geojson\"))\n",
    "\n",
    "    print(f\"Merging {len(geojson_files)} patch files for slide: {slide_name}\")\n",
    "    \n",
    "    for geojson_file in tqdm(geojson_files, desc=\"ðŸ”„ Merging GeoJSONs\"):\n",
    "        try:\n",
    "            with open(geojson_file, \"r\") as f:\n",
    "                data = geojson.load(f)\n",
    "\n",
    "            if not isinstance(data, FeatureCollection):\n",
    "                print(f\"Skipping invalid GeoJSON file: {geojson_file.name}\")\n",
    "                continue\n",
    "\n",
    "            for feature in data.features:\n",
    "                try:\n",
    "                    geometry = shape(feature.geometry)\n",
    "                    if not geometry.is_valid:\n",
    "                        geometry = geometry.buffer(0)  # Try to fix\n",
    "                    if not geometry.is_valid:\n",
    "                        print(f\"Still invalid geometry in: {geojson_file.name}\")\n",
    "                        continue\n",
    "                    feature.geometry = mapping(geometry)  # Update to fixed\n",
    "\n",
    "                except Exception as gex:\n",
    "                    print(f\"Geometry error in {geojson_file.name}: {gex}\")\n",
    "                    continue\n",
    "\n",
    "                feature.properties.setdefault(\"objectType\", \"annotation\")\n",
    "\n",
    "                classification = feature.properties.get(\"classification\", {})\n",
    "                class_name = classification.get(\"name\", \"unknown\").lower()\n",
    "                source_type = feature.properties.get(\"source\", \"unknown\")\n",
    "\n",
    "                if class_name == \"eos\":\n",
    "                    if source_type == \"bbox\":\n",
    "                        classification[\"color\"] = [1.0, 0.0, 0.0]\n",
    "                    elif source_type == \"polygon\":\n",
    "                        classification[\"color\"] = [1.0, 0.0, 0.0]\n",
    "                        classification[\"fillColor\"] = [1.0, 0.0, 0.0, 0.3]\n",
    "                elif class_name == \"eosg\":\n",
    "                    if source_type == \"bbox\":\n",
    "                        classification[\"color\"] = [0.0, 1.0, 0.0]\n",
    "                    elif source_type == \"polygon\":\n",
    "                        classification[\"color\"] = [0.0, 1.0, 0.0]\n",
    "                        classification[\"fillColor\"] = [0.0, 1.0, 0.0, 0.3]\n",
    "                else:\n",
    "                    classification.setdefault(\"color\", [0.666, 0.666, 0.666])\n",
    "\n",
    "                feature.properties[\"classification\"] = classification\n",
    "                feature.properties[\"source_patch\"] = geojson_file.name.replace(\"_detections.geojson\", \"\")\n",
    "\n",
    "                merged_features.append(feature)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {geojson_file.name}: {e}\")\n",
    "\n",
    "    if not merged_features:\n",
    "        print(f\"No features found to merge for slide: {slide_name}\")\n",
    "        return\n",
    "\n",
    "    merged_geojson = FeatureCollection(merged_features)\n",
    "    merged_path = slide_output_dir / f\"{slide_name}_detections_merged.geojson\"\n",
    "\n",
    "    with open(merged_path, \"w\") as out_f:\n",
    "        geojson.dump(to_serializable(merged_geojson), out_f, indent=2)\n",
    "\n",
    "    print(f\"\\nMerged GeoJSON saved: {merged_path}\")\n",
    "    print(f\"Individual patch GeoJSON files preserved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a80df4-8116-48d2-8845-0002904ae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide.close()\n",
    "# print(f\"Finished processing slide: {slide_name}\")\n",
    "\n",
    "# CODE FOR MERGING THE LAST PROCESSED SVS FILE!\n",
    "# If you want to merge all the individual GeoJSON files for each svs in subfolders inside inference10 folder, please, use the next cells below\n",
    "merge_geojsons(slide_output_dir, slide_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cfe52-11d8-4cd9-8dcb-fd7572b17355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging GeoJSON in individual folder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfc79b-dc44-408d-b08e-0343b63e6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# # Replace with the actual folder path\n",
    "# slide_output_dir = Path(\"inference_output10\") / \"1007439\"\n",
    "# slide_name = \"1007439\"\n",
    "\n",
    "# merge_geojsons(slide_output_dir, slide_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bacc4-4818-425f-96e9-4c39b52fa824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge All Slides in inference_output10 if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e5006-9b7c-4fcb-bcd6-21fbec96d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch Merge with Skipping Existing Merged Files\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Path to a top-level inference output directory\n",
    "# root_dir = Path(\"inference_output10\")\n",
    "\n",
    "# # Go through each subfolder (each slide)\n",
    "# for slide_dir in sorted(root_dir.iterdir()):\n",
    "#     if slide_dir.is_dir():\n",
    "#         slide_name = slide_dir.name\n",
    "#         merged_file = slide_dir / f\"{slide_name}_detections_merged.geojson\"\n",
    "\n",
    "#         if merged_file.exists():\n",
    "#             print(f\"Skipping '{slide_name}': merged file already exists.\")\n",
    "#             continue\n",
    "\n",
    "#         print(f\"\\n Processing slide: {slide_name}\")\n",
    "#         merge_geojsons(slide_dir, slide_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc940574-6a14-480b-8aaa-996573145b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: remove temporary images\n",
    "# Comment the lines below if you wanr to keep temporary images for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69560374-750a-41a3-a27a-509dabd39f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: remove temporary images\n",
    "temp_dir = Path(\"temp_inference_patches\")\n",
    "removed_files = 0  # Counter for removed files\n",
    "\n",
    "for f in temp_dir.glob(\"*.jpg\"):\n",
    "    try:\n",
    "        f.unlink()  # Attempt to delete the file\n",
    "        removed_files += 1  # Increment counter if file is successfully removed\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete {f.name}: {e}\")\n",
    "\n",
    "# Print the final cleanup message\n",
    "print(f\"All temporary files removed. {removed_files} file(s) deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78851168-5a9d-4961-af87-4261d638299f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
